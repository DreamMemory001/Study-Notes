# 线性回归

* 线性回归是回归问题，通过给定训练集m，选择一个模型函数h，通过计算得到模型函数的最优解，并计算最优解下的参数。

* 线性回归分为：

  * 单变量线性回归：
    $$
    h(x) = θ0 + θ1*x1
    $$
    

  * 多变量线性回归：
    $$
    h(x) = θ0 + θ1*x1 + θ2*x2 + θ3*x3
    $$
    

* 通用表达式：
  $$
  h(x) = θT*X = θ0*x0 + θ1*x1 + ... + θn*xn
  $$
  

* 代价函数：代价函数是计算建立模型和真实数据的误差。

  ![1531620822180](C:\Users\16500\AppData\Local\Temp\1531620822180.png)

  * m：训练样本个数
  * n：训练集的特征个数

### 正规方程

* 正规方程是通过求解下面的方程来找出使得代价函数最小的参数的：

  ![](C:\Users\16500\Desktop\F0QPL8BJSLT52A}N95PD]GG.png)

  用来求解θ的最小值。
  $$
  θ = (XT*X)-1*XT*y
  $$

  * x：训练集矩阵：(mx(n+1))
  * y：训练集对应的正确答案：(1xm)

* 注意：矩阵不可逆不能使用正规方程（原因：1.各个特征不独立 2.特征数量大于样本个数）

### 梯度下降算法（局部最小值）

* 梯度下降算法是计算达到最优解时，代价函数最小值时的θ值。`每次选取所有样本更新θ`，α指学习率。

![](C:\Users\16500\Desktop\ZJD74IZ]UK4RZX10}VUNWRU.png)

* 代入J(θ)，得

![](C:\Users\16500\Desktop\2W5{4@W9D`6SSQ86Y%YAM@9.png)



* 求导后得到

  

![](C:\Users\16500\Desktop\%NK]8FA0LGP8NT_LE7PLW7C.png)

* 每个θj必须是同步变换。



### 特征缩放

* 对于多个特征值相差较大时，需要特征缩放，把所有参数缩放到-1~1的范围，让x适应每个参数。也就是
  $$
  Xn = (Xn - μn)/Sn
  $$

  * μ是平均值，S是标准差。

### 随机梯度下降算法

* 每次只取一个样本更新θ。

* 常用学习率α：`0.01`、`0.03`、`0.1`、`0.3`、`1`、`3`、`10`。

* 梯度下降算法与正规方程的比较：

  | 梯度下降算法          | 正规方程         |
  | --------------------- | ---------------- |
  | 需要选择学习率α       | 不需要           |
  | 需多次迭代            | 一次运算得出     |
  | 特征数量n较大时也适用 | n ≤ 10000        |
  | 适用于各种模型        | 只适用于线性模型 |

  